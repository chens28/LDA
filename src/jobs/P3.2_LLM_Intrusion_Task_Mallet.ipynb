{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Using cached langchain-0.1.16-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting PyYAML>=5.3 (from langchain)\n",
      "  Using cached PyYAML-6.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Using cached SQLAlchemy-2.0.29-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
      "  Using cached aiohttp-3.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
      "  Using cached dataclasses_json-0.6.4-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langchain-community<0.1,>=0.0.32 (from langchain)\n",
      "  Using cached langchain_community-0.0.34-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting langchain-core<0.2.0,>=0.1.42 (from langchain)\n",
      "  Using cached langchain_core-0.1.46-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
      "  Using cached langchain_text_splitters-0.0.1-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
      "  Using cached langsmith-0.1.51-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting numpy<2,>=1 (from langchain)\n",
      "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting pydantic<3,>=1 (from langchain)\n",
      "  Using cached pydantic-2.7.1-py3-none-any.whl.metadata (107 kB)\n",
      "Collecting requests<3,>=2 (from langchain)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tenacity<9.0.0,>=8.1.0 (from langchain)\n",
      "  Using cached tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached frozenlist-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached multidict-6.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached yarl-1.9.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Using cached marshmallow-3.21.1-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
      "  Using cached jsonpointer-2.4-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.42->langchain)\n",
      "  Using cached packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Using cached orjson-3.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1->langchain)\n",
      "  Using cached annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.18.2 (from pydantic<3,>=1->langchain)\n",
      "  Using cached pydantic_core-2.18.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting typing-extensions>=4.6.1 (from pydantic<3,>=1->langchain)\n",
      "  Using cached typing_extensions-4.11.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2->langchain)\n",
      "  Using cached charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2->langchain)\n",
      "  Using cached idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2->langchain)\n",
      "  Using cached urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2->langchain)\n",
      "  Using cached certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Using cached greenlet-3.0.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Using cached langchain-0.1.16-py3-none-any.whl (817 kB)\n",
      "Using cached aiohttp-3.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Using cached dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached langchain_community-0.0.34-py3-none-any.whl (1.9 MB)\n",
      "Using cached langchain_core-0.1.46-py3-none-any.whl (299 kB)\n",
      "Using cached langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
      "Using cached langsmith-0.1.51-py3-none-any.whl (115 kB)\n",
      "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "Using cached pydantic-2.7.1-py3-none-any.whl (409 kB)\n",
      "Using cached pydantic_core-2.18.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "Using cached PyYAML-6.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (757 kB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached SQLAlchemy-2.0.29-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "Using cached tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (140 kB)\n",
      "Using cached frozenlist-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (272 kB)\n",
      "Using cached greenlet-3.0.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (620 kB)\n",
      "Using cached idna-3.7-py3-none-any.whl (66 kB)\n",
      "Using cached jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
      "Using cached marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
      "Using cached multidict-6.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (128 kB)\n",
      "Using cached orjson-3.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
      "Using cached packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Using cached typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "Using cached yarl-1.9.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (328 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: urllib3, typing-extensions, tenacity, PyYAML, packaging, orjson, numpy, mypy-extensions, multidict, jsonpointer, idna, greenlet, frozenlist, charset-normalizer, certifi, attrs, annotated-types, yarl, typing-inspect, SQLAlchemy, requests, pydantic-core, marshmallow, jsonpatch, aiosignal, pydantic, dataclasses-json, aiohttp, langsmith, langchain-core, langchain-text-splitters, langchain-community, langchain\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda-repo-cli 1.0.75 requires requests_mock, which is not installed.\n",
      "gensim 4.3.0 requires FuzzyTM>=0.4.0, which is not installed.\n",
      "conda-repo-cli 1.0.75 requires clyent==1.2.1, but you have clyent 1.2.2 which is incompatible.\n",
      "anaconda-cloud-auth 0.1.4 requires pydantic<2.0, but you have pydantic 2.7.1 which is incompatible.\n",
      "botocore 1.31.64 requires urllib3<2.1,>=1.25.4; python_version >= \"3.10\", but you have urllib3 2.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed PyYAML-6.0.1 SQLAlchemy-2.0.29 aiohttp-3.9.5 aiosignal-1.3.1 annotated-types-0.6.0 attrs-23.2.0 certifi-2024.2.2 charset-normalizer-3.3.2 dataclasses-json-0.6.4 frozenlist-1.4.1 greenlet-3.0.3 idna-3.7 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.16 langchain-community-0.0.34 langchain-core-0.1.46 langchain-text-splitters-0.0.1 langsmith-0.1.51 marshmallow-3.21.1 multidict-6.0.5 mypy-extensions-1.0.0 numpy-1.26.4 orjson-3.10.1 packaging-23.2 pydantic-2.7.1 pydantic-core-2.18.2 requests-2.31.0 tenacity-8.2.3 typing-extensions-4.11.0 typing-inspect-0.9.0 urllib3-2.2.1 yarl-1.9.4\n",
      "\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/urllib3 already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/urllib3-2.2.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/typing_extensions.py already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/typing_extensions-4.11.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/__pycache__ already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/tenacity already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/tenacity-8.2.3.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/yaml already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/_yaml already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/PyYAML-6.0.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/packaging already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/packaging-23.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/orjson-3.10.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/orjson already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/numpy already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/numpy-1.26.4.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/numpy.libs already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/mypy_extensions.py already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/mypy_extensions-1.0.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/multidict-6.0.5.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/multidict already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/jsonpointer.py already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/jsonpointer-2.4.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/idna already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/idna-3.7.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/greenlet already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/greenlet-3.0.3.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/frozenlist-1.4.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/frozenlist already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/charset_normalizer already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/charset_normalizer-3.3.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/certifi already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/certifi-2024.2.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/attr already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/attrs already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/attrs-23.2.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/annotated_types already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/annotated_types-0.6.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/yarl already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/yarl-1.9.4.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/typing_inspect.py already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/typing_inspect-0.9.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/sqlalchemy already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/SQLAlchemy-2.0.29.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/requests already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/requests-2.31.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/pydantic_core-2.18.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/pydantic_core already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/marshmallow already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/marshmallow-3.21.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/jsonpatch.py already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/jsonpatch-1.33.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/aiosignal already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/aiosignal-1.3.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/pydantic already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/pydantic-2.7.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/dataclasses_json already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/dataclasses_json-0.6.4.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/aiohttp-3.9.5.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/aiohttp already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/langsmith already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/langsmith-0.1.51.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/langchain_core already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/langchain_core-0.1.46.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/langchain_text_splitters already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/langchain_text_splitters-0.0.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/langchain_community already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/langchain_community-0.0.34.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/langchain already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/langchain-0.1.16.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/bin already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/include already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install langchain --target /home/aah9103/.local/lib/python3.11/site-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdf2image\n",
      "  Using cached pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting pillow (from pdf2image)\n",
      "  Using cached pillow-10.3.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
      "Using cached pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
      "Using cached pillow-10.3.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "Installing collected packages: pillow, pdf2image\n",
      "Successfully installed pdf2image-1.17.0 pillow-10.3.0\n",
      "\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/pillow.libs already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/PIL already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/pillow-10.3.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/pdf2image already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/pdf2image-1.17.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pdf2image --target /home/aah9103/.local/lib/python3.11/site-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting GPT4All\n",
      "  Using cached gpt4all-2.6.0-py3-none-manylinux1_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting requests (from GPT4All)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm (from GPT4All)\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->GPT4All)\n",
      "  Using cached charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->GPT4All)\n",
      "  Using cached idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->GPT4All)\n",
      "  Using cached urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->GPT4All)\n",
      "  Using cached certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Using cached gpt4all-2.6.0-py3-none-manylinux1_x86_64.whl (3.9 MB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (140 kB)\n",
      "Using cached idna-3.7-py3-none-any.whl (66 kB)\n",
      "Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "Installing collected packages: urllib3, tqdm, idna, charset-normalizer, certifi, requests, GPT4All\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda-repo-cli 1.0.75 requires requests_mock, which is not installed.\n",
      "conda-repo-cli 1.0.75 requires clyent==1.2.1, but you have clyent 1.2.2 which is incompatible.\n",
      "anaconda-cloud-auth 0.1.4 requires pydantic<2.0, but you have pydantic 2.7.1 which is incompatible.\n",
      "botocore 1.31.64 requires urllib3<2.1,>=1.25.4; python_version >= \"3.10\", but you have urllib3 2.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed GPT4All-2.6.0 certifi-2024.2.2 charset-normalizer-3.3.2 idna-3.7 requests-2.31.0 tqdm-4.66.2 urllib3-2.2.1\n",
      "\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/urllib3 already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/urllib3-2.2.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/tqdm already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/tqdm-4.66.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/idna already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/idna-3.7.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/charset_normalizer already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/charset_normalizer-3.3.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/certifi already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/certifi-2024.2.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/requests already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/requests-2.31.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/gpt4all already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/gpt4all-2.6.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aah9103/.local/lib/python3.11/site-packages/bin already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install GPT4All --target /home/aah9103/.local/lib/python3.11/site-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/share/apps/anaconda3/2024.02/bin/python'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/aah9103',\n",
       " '/share/apps/anaconda3/2024.02/lib/python311.zip',\n",
       " '/share/apps/anaconda3/2024.02/lib/python3.11',\n",
       " '/share/apps/anaconda3/2024.02/lib/python3.11/lib-dynload',\n",
       " '',\n",
       " '/home/aah9103/.local/lib/python3.11/site-packages',\n",
       " '/share/apps/anaconda3/2024.02/lib/python3.11/site-packages']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import GPT4All\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from pdf2image import convert_from_path\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Load dictionary:\n",
    "# data = {0: {'top 5 sample': ['water', 'area', 'river', 'park', 'miles'], 'intruder': 'game'}, \n",
    "#         1: {'top 5 sample': ['horses', 'horse', 'breed', 'coins', 'silver'], 'intruder': 'hindu'},\n",
    "#         2: {'top 5 sample': ['issue', 'situation', 'lead', 'stick', 'nice'], 'intruder': 'tampon'}}\n",
    "\n",
    "# #gensim VI is worse than mallet GS \n",
    "# #10-20-50 topics. regenerate 20 times(mutlitple runs) and then over multiple datasetsg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Gensim File\n",
    "dataG = pd.read_csv('gensim_test.csv')\n",
    "dataG['package'] = 'gensim'\n",
    "#dataG.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Mallet file\n",
    "dataM = pd.read_csv('mallet_test.csv')\n",
    "dataM['package'] = 'mallet'\n",
    "#dataM.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shorten for test\n",
    "dataM = dataM.head(1)\n",
    "dataG = dataG.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_topics</th>\n",
       "      <th>seed</th>\n",
       "      <th>score</th>\n",
       "      <th>interval</th>\n",
       "      <th>burnin</th>\n",
       "      <th>tests</th>\n",
       "      <th>package</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>7097</td>\n",
       "      <td>0.619082</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>{0: {'top 5 sample': ['milk', 'feed', 'month',...</td>\n",
       "      <td>mallet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>5301</td>\n",
       "      <td>0.632168</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>{0: {'top 5 sample': ['pregnant', 'day', 'week...</td>\n",
       "      <td>mallet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>8687</td>\n",
       "      <td>0.638164</td>\n",
       "      <td>100</td>\n",
       "      <td>500</td>\n",
       "      <td>{0: {'top 5 sample': ['breastfeed', 'milk', 'f...</td>\n",
       "      <td>mallet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>3433</td>\n",
       "      <td>0.624600</td>\n",
       "      <td>100</td>\n",
       "      <td>500</td>\n",
       "      <td>{0: {'top 5 sample': ['time', 'make', 'orgasm'...</td>\n",
       "      <td>mallet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>511</td>\n",
       "      <td>0.626687</td>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "      <td>{0: {'top 5 sample': ['abortion', 'mother', 'f...</td>\n",
       "      <td>mallet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>7097</td>\n",
       "      <td>0.646652</td>\n",
       "      <td>100</td>\n",
       "      <td>500</td>\n",
       "      <td>{0: {'top 5 sample': ['day', 'make', 'help', '...</td>\n",
       "      <td>mallet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20</td>\n",
       "      <td>5301</td>\n",
       "      <td>0.645397</td>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "      <td>{0: {'top 5 sample': ['time', 'start', 'feel',...</td>\n",
       "      <td>mallet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20</td>\n",
       "      <td>8687</td>\n",
       "      <td>0.644444</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>{0: {'top 5 sample': ['baby', 'epidural', 'con...</td>\n",
       "      <td>mallet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20</td>\n",
       "      <td>3433</td>\n",
       "      <td>0.632472</td>\n",
       "      <td>100</td>\n",
       "      <td>500</td>\n",
       "      <td>{0: {'top 5 sample': ['feel', 'make', 'pregnan...</td>\n",
       "      <td>mallet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20</td>\n",
       "      <td>511</td>\n",
       "      <td>0.646027</td>\n",
       "      <td>100</td>\n",
       "      <td>500</td>\n",
       "      <td>{0: {'top 5 sample': ['gain', 'pregnancy', 'we...</td>\n",
       "      <td>mallet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>50</td>\n",
       "      <td>7097</td>\n",
       "      <td>0.639054</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>{0: {'top 5 sample': ['start', 'baby', 'day', ...</td>\n",
       "      <td>mallet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>50</td>\n",
       "      <td>5301</td>\n",
       "      <td>0.640971</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>{0: {'top 5 sample': ['test', 'problem', 'doct...</td>\n",
       "      <td>mallet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>50</td>\n",
       "      <td>8687</td>\n",
       "      <td>0.641022</td>\n",
       "      <td>100</td>\n",
       "      <td>500</td>\n",
       "      <td>{0: {'top 5 sample': ['human', 'kill', 'person...</td>\n",
       "      <td>mallet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>50</td>\n",
       "      <td>3433</td>\n",
       "      <td>0.643896</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>{0: {'top 5 sample': ['antibiotic', 'yeast', '...</td>\n",
       "      <td>mallet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>50</td>\n",
       "      <td>511</td>\n",
       "      <td>0.635056</td>\n",
       "      <td>50</td>\n",
       "      <td>500</td>\n",
       "      <td>{0: {'top 5 sample': ['pregnancy', 'stress', '...</td>\n",
       "      <td>mallet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>100</td>\n",
       "      <td>7097</td>\n",
       "      <td>0.617878</td>\n",
       "      <td>100</td>\n",
       "      <td>500</td>\n",
       "      <td>{0: {'top 5 sample': ['sexual_harassment', 'ph...</td>\n",
       "      <td>mallet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>100</td>\n",
       "      <td>5301</td>\n",
       "      <td>0.610395</td>\n",
       "      <td>50</td>\n",
       "      <td>200</td>\n",
       "      <td>{0: {'top 5 sample': ['cervix', 'day', 'week',...</td>\n",
       "      <td>mallet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>100</td>\n",
       "      <td>8687</td>\n",
       "      <td>0.619984</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>{0: {'top 5 sample': ['feel', 'lot', 'hurt', '...</td>\n",
       "      <td>mallet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>100</td>\n",
       "      <td>3433</td>\n",
       "      <td>0.616891</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>{0: {'top 5 sample': ['pregnancy', 'baby', 'be...</td>\n",
       "      <td>mallet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>100</td>\n",
       "      <td>511</td>\n",
       "      <td>0.620777</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>{0: {'top 5 sample': ['circumcision', 'cut', '...</td>\n",
       "      <td>mallet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    num_topics  seed     score  interval  burnin  \\\n",
       "0           10  7097  0.619082         5     100   \n",
       "1           10  5301  0.632168       100     100   \n",
       "2           10  8687  0.638164       100     500   \n",
       "3           10  3433  0.624600       100     500   \n",
       "4           10   511  0.626687       100     200   \n",
       "5           20  7097  0.646652       100     500   \n",
       "6           20  5301  0.645397       100     200   \n",
       "7           20  8687  0.644444        10     100   \n",
       "8           20  3433  0.632472       100     500   \n",
       "9           20   511  0.646027       100     500   \n",
       "10          50  7097  0.639054       100     100   \n",
       "11          50  5301  0.640971         5     100   \n",
       "12          50  8687  0.641022       100     500   \n",
       "13          50  3433  0.643896        50      50   \n",
       "14          50   511  0.635056        50     500   \n",
       "15         100  7097  0.617878       100     500   \n",
       "16         100  5301  0.610395        50     200   \n",
       "17         100  8687  0.619984        50     100   \n",
       "18         100  3433  0.616891       100     100   \n",
       "19         100   511  0.620777       100     100   \n",
       "\n",
       "                                                tests package  \n",
       "0   {0: {'top 5 sample': ['milk', 'feed', 'month',...  mallet  \n",
       "1   {0: {'top 5 sample': ['pregnant', 'day', 'week...  mallet  \n",
       "2   {0: {'top 5 sample': ['breastfeed', 'milk', 'f...  mallet  \n",
       "3   {0: {'top 5 sample': ['time', 'make', 'orgasm'...  mallet  \n",
       "4   {0: {'top 5 sample': ['abortion', 'mother', 'f...  mallet  \n",
       "5   {0: {'top 5 sample': ['day', 'make', 'help', '...  mallet  \n",
       "6   {0: {'top 5 sample': ['time', 'start', 'feel',...  mallet  \n",
       "7   {0: {'top 5 sample': ['baby', 'epidural', 'con...  mallet  \n",
       "8   {0: {'top 5 sample': ['feel', 'make', 'pregnan...  mallet  \n",
       "9   {0: {'top 5 sample': ['gain', 'pregnancy', 'we...  mallet  \n",
       "10  {0: {'top 5 sample': ['start', 'baby', 'day', ...  mallet  \n",
       "11  {0: {'top 5 sample': ['test', 'problem', 'doct...  mallet  \n",
       "12  {0: {'top 5 sample': ['human', 'kill', 'person...  mallet  \n",
       "13  {0: {'top 5 sample': ['antibiotic', 'yeast', '...  mallet  \n",
       "14  {0: {'top 5 sample': ['pregnancy', 'stress', '...  mallet  \n",
       "15  {0: {'top 5 sample': ['sexual_harassment', 'ph...  mallet  \n",
       "16  {0: {'top 5 sample': ['cervix', 'day', 'week',...  mallet  \n",
       "17  {0: {'top 5 sample': ['feel', 'lot', 'hurt', '...  mallet  \n",
       "18  {0: {'top 5 sample': ['pregnancy', 'baby', 'be...  mallet  \n",
       "19  {0: {'top 5 sample': ['circumcision', 'cut', '...  mallet  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data = pd.concat([dataG, dataM], ignore_index=True)#.reset_index()\n",
    "data = dataM\n",
    "data.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #load json output of topic model\n",
    "# with open('llm_tests.json', 'r') as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "# f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fix JSON strings and convert them to JSON objects\n",
    "def convertJSON(s):\n",
    "    s = s.replace(\"'\", '\"')\n",
    "    s = re.sub(r'(\\s*)(\\d+)(\\s*):', r'\\1\"\\2\"\\3:', s)\n",
    "    return json.loads(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load json string\n",
    "data['tests'] = data['tests'].apply(lambda x: convertJSON(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #load json output of topic model\n",
    "# with open('gensim_tests.json', 'r') as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "# f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = GPT4All(model=\"./mistral-7b-openorca.gguf2.Q4_0.gguf\", backend = 'gptj', verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are a helpful assistant evaluating the top words of a topic model output for a given topic. \n",
    "Select which word or phrase is the least related to all other words or phrases in terms of the meaning of the word or phrase. \n",
    "If multiple words do not fit, choose the word that is most out of place in terms of the meaning of the word.\n",
    "{datasetDescription}\n",
    "Here is the list of words for your task: [{listOfWords}]\n",
    "Your answer MUST CONTAIN ONLY a single word.\n",
    "Also, mark the selected word by preceding it with 000 and succeeding it with 000.\n",
    "For example if a list of words is \"apple, banana, cat\", your response should ONLY BE: '000cat000'\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(template = template, input_variables = ['datasetDescription', 'listOfWords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "llmChain = LLMChain(prompt = prompt, llm = llm, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ans = llmChain({'datasetDescription': 'data of words', 'listOfWords': 'apple, banana, cat, sheep, house'})\n",
    "\n",
    "# regSearch = re.search(r'000(.*?)000', ans['text'])\n",
    "# if regSearch:\n",
    "#     print(regSearch.group(1))\n",
    "# else:\n",
    "#     print('no match')\n",
    "\n",
    "# You are a helpful assistant evaluating the top words of a topic model output for a given topic. \n",
    "# Select which word or phrase is the least related to all other words or phrases in terms of the meaning of the word or phrase. \n",
    "# If multiple words do not fit, choose the word that is most out of place in terms of the meaning of the word.\n",
    "# Pay attention to the similarity of each word to all othe words in the set when you make your decision.\n",
    "# Here is the list of words for your task: [hour, gain, week, baby, labor, section]\n",
    "# Your answer MUST CONTAIN ONLY a single word.\n",
    "# Also, mark the selected word by preceding it with 000 and succeeding it with 000.\n",
    "# For example if a list of words is \"apple, banana, cat\", your response should ONLY BE: '000cat000'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDesc = \"\"\"\n",
    "Pay attention to the similarity of each word to all other words in the set when you make your decision.\n",
    "\"\"\"\n",
    "\n",
    "# dataDesc = \"\"\"\n",
    "# The topic modelling is based on a corpus of reddit posts relating to maternal health. \n",
    "# The data from which topic words are extracted includes topics like abortion and sexuality.\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k, v in data.items():\n",
    "#     wordList = v['top 5 sample'] + [v['intruder']]\n",
    "#     random.shuffle(wordList)\n",
    "#     wordListStr = \", \".join(wordList)\n",
    "#     trueIntruder = v['intruder']\n",
    "#     detectedIntruder = 'NOTDETECTED'\n",
    "#     ans = llmChain({'datasetDescription': dataDesc, 'listOfWords': wordListStr})\n",
    "#     regSearch = re.search(r'000(.*?)000', ans['text'])\n",
    "#     data[k]['wordlist'] = wordList\n",
    "#     data[k]['wordliststr'] = wordListStr\n",
    "#     if regSearch:\n",
    "#         detectedIntruder = regSearch.group(1)\n",
    "#     data[k]['detectedIntruder'] = detectedIntruder\n",
    "#     if trueIntruder.lower() == detectedIntruder.lower():\n",
    "#         data[k]['success'] = 1\n",
    "#     else:\n",
    "#         data[k]['success'] = 0\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame.from_dict(data, orient='index')\n",
    "# rate = df['success'].sum() / len(df)\n",
    "# rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llmEval(data, llmChain, maxIter = 10, dataDesc = \"\"\"\"\"\"):\n",
    "    tot = len(data)\n",
    "    i = 1\n",
    "    for k, v in data.items():\n",
    "        print(\"Evaluating \",i,\" / \",tot)\n",
    "        i += 1\n",
    "        itr = maxIter\n",
    "        wordList = v['top 5 sample'] + [v['intruder']]\n",
    "        random.shuffle(wordList)\n",
    "        wordListStr = \", \".join(wordList)\n",
    "        trueIntruder = v['intruder']\n",
    "        data[k]['wordlist'] = wordList\n",
    "        data[k]['wordliststr'] = wordListStr\n",
    "        detectedIntruder = 'NOTDETECTED'\n",
    "        while itr > 0:\n",
    "            itr -= 1\n",
    "            if detectedIntruder != 'NOTDETECTED':\n",
    "                itr = 0\n",
    "                break\n",
    "            ans = llmChain({'datasetDescription': dataDesc, 'listOfWords': wordListStr})\n",
    "            regSearch = re.search(r'000(.*?)000', ans['text'])\n",
    "            if regSearch:\n",
    "                detectedIntruder = regSearch.group(1)\n",
    "\n",
    "        data[k]['detectedIntruder'] = detectedIntruder\n",
    "        data[k]['rawLLMOP'] = ans['text']\n",
    "        if trueIntruder.lower() == detectedIntruder.lower():\n",
    "            data[k]['success'] = 1\n",
    "        else:\n",
    "            data[k]['success'] = 0\n",
    "\n",
    "    df = pd.DataFrame.from_dict(data, orient='index')\n",
    "    rate = df['success'].sum() / len(df)\n",
    "    return pd.Series({'score': rate, 'data': data})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating  1  /  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aah9103/.local/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating  2  /  10\n",
      "Evaluating  3  /  10\n",
      "Evaluating  4  /  10\n",
      "Evaluating  5  /  10\n",
      "Evaluating  6  /  10\n",
      "Evaluating  7  /  10\n",
      "Evaluating  8  /  10\n",
      "Evaluating  9  /  10\n",
      "Evaluating  10  /  10\n",
      "Evaluating  1  /  10\n",
      "Evaluating  2  /  10\n",
      "Evaluating  3  /  10\n",
      "Evaluating  4  /  10\n",
      "Evaluating  5  /  10\n",
      "Evaluating  6  /  10\n",
      "Evaluating  7  /  10\n",
      "Evaluating  8  /  10\n",
      "Evaluating  9  /  10\n",
      "Evaluating  10  /  10\n",
      "Evaluating  1  /  10\n",
      "Evaluating  2  /  10\n",
      "Evaluating  3  /  10\n",
      "Evaluating  4  /  10\n",
      "Evaluating  5  /  10\n",
      "Evaluating  6  /  10\n",
      "Evaluating  7  /  10\n",
      "Evaluating  8  /  10\n",
      "Evaluating  9  /  10\n",
      "Evaluating  10  /  10\n",
      "Evaluating  1  /  10\n",
      "Evaluating  2  /  10\n",
      "Evaluating  3  /  10\n",
      "Evaluating  4  /  10\n",
      "Evaluating  5  /  10\n",
      "Evaluating  6  /  10\n",
      "Evaluating  7  /  10\n",
      "Evaluating  8  /  10\n",
      "Evaluating  9  /  10\n",
      "Evaluating  10  /  10\n",
      "Evaluating  1  /  10\n",
      "Evaluating  2  /  10\n",
      "Evaluating  3  /  10\n",
      "Evaluating  4  /  10\n",
      "Evaluating  5  /  10\n",
      "Evaluating  6  /  10\n",
      "Evaluating  7  /  10\n",
      "Evaluating  8  /  10\n",
      "Evaluating  9  /  10\n",
      "Evaluating  10  /  10\n",
      "Evaluating  1  /  20\n",
      "Evaluating  2  /  20\n",
      "Evaluating  3  /  20\n",
      "Evaluating  4  /  20\n",
      "Evaluating  5  /  20\n",
      "Evaluating  6  /  20\n",
      "Evaluating  7  /  20\n",
      "Evaluating  8  /  20\n",
      "Evaluating  9  /  20\n",
      "Evaluating  10  /  20\n",
      "Evaluating  11  /  20\n",
      "Evaluating  12  /  20\n",
      "Evaluating  13  /  20\n",
      "Evaluating  14  /  20\n",
      "Evaluating  15  /  20\n",
      "Evaluating  16  /  20\n",
      "Evaluating  17  /  20\n",
      "Evaluating  18  /  20\n",
      "Evaluating  19  /  20\n",
      "Evaluating  20  /  20\n",
      "Evaluating  1  /  20\n",
      "Evaluating  2  /  20\n",
      "Evaluating  3  /  20\n",
      "Evaluating  4  /  20\n",
      "Evaluating  5  /  20\n",
      "Evaluating  6  /  20\n",
      "Evaluating  7  /  20\n",
      "Evaluating  8  /  20\n",
      "Evaluating  9  /  20\n",
      "Evaluating  10  /  20\n",
      "Evaluating  11  /  20\n",
      "Evaluating  12  /  20\n",
      "Evaluating  13  /  20\n",
      "Evaluating  14  /  20\n",
      "Evaluating  15  /  20\n",
      "Evaluating  16  /  20\n",
      "Evaluating  17  /  20\n",
      "Evaluating  18  /  20\n",
      "Evaluating  19  /  20\n",
      "Evaluating  20  /  20\n",
      "Evaluating  1  /  20\n",
      "Evaluating  2  /  20\n",
      "Evaluating  3  /  20\n",
      "Evaluating  4  /  20\n",
      "Evaluating  5  /  20\n",
      "Evaluating  6  /  20\n",
      "Evaluating  7  /  20\n",
      "Evaluating  8  /  20\n",
      "Evaluating  9  /  20\n",
      "Evaluating  10  /  20\n",
      "Evaluating  11  /  20\n",
      "Evaluating  12  /  20\n",
      "Evaluating  13  /  20\n",
      "Evaluating  14  /  20\n",
      "Evaluating  15  /  20\n",
      "Evaluating  16  /  20\n",
      "Evaluating  17  /  20\n",
      "Evaluating  18  /  20\n",
      "Evaluating  19  /  20\n",
      "Evaluating  20  /  20\n",
      "Evaluating  1  /  20\n",
      "Evaluating  2  /  20\n",
      "Evaluating  3  /  20\n",
      "Evaluating  4  /  20\n",
      "Evaluating  5  /  20\n",
      "Evaluating  6  /  20\n",
      "Evaluating  7  /  20\n",
      "Evaluating  8  /  20\n",
      "Evaluating  9  /  20\n",
      "Evaluating  10  /  20\n",
      "Evaluating  11  /  20\n",
      "Evaluating  12  /  20\n",
      "Evaluating  13  /  20\n",
      "Evaluating  14  /  20\n",
      "Evaluating  15  /  20\n",
      "Evaluating  16  /  20\n",
      "Evaluating  17  /  20\n",
      "Evaluating  18  /  20\n",
      "Evaluating  19  /  20\n",
      "Evaluating  20  /  20\n",
      "Evaluating  1  /  20\n",
      "Evaluating  2  /  20\n",
      "Evaluating  3  /  20\n",
      "Evaluating  4  /  20\n",
      "Evaluating  5  /  20\n",
      "Evaluating  6  /  20\n",
      "Evaluating  7  /  20\n",
      "Evaluating  8  /  20\n",
      "Evaluating  9  /  20\n",
      "Evaluating  10  /  20\n",
      "Evaluating  11  /  20\n",
      "Evaluating  12  /  20\n",
      "Evaluating  13  /  20\n",
      "Evaluating  14  /  20\n",
      "Evaluating  15  /  20\n",
      "Evaluating  16  /  20\n",
      "Evaluating  17  /  20\n",
      "Evaluating  18  /  20\n",
      "Evaluating  19  /  20\n",
      "Evaluating  20  /  20\n",
      "Evaluating  1  /  50\n",
      "Evaluating  2  /  50\n",
      "Evaluating  3  /  50\n",
      "Evaluating  4  /  50\n",
      "Evaluating  5  /  50\n",
      "Evaluating  6  /  50\n",
      "Evaluating  7  /  50\n",
      "Evaluating  8  /  50\n",
      "Evaluating  9  /  50\n",
      "Evaluating  10  /  50\n",
      "Evaluating  11  /  50\n",
      "Evaluating  12  /  50\n",
      "Evaluating  13  /  50\n",
      "Evaluating  14  /  50\n",
      "Evaluating  15  /  50\n",
      "Evaluating  16  /  50\n",
      "Evaluating  17  /  50\n",
      "Evaluating  18  /  50\n",
      "Evaluating  19  /  50\n",
      "Evaluating  20  /  50\n",
      "Evaluating  21  /  50\n",
      "Evaluating  22  /  50\n",
      "Evaluating  23  /  50\n",
      "Evaluating  24  /  50\n",
      "Evaluating  25  /  50\n",
      "Evaluating  26  /  50\n",
      "Evaluating  27  /  50\n",
      "Evaluating  28  /  50\n",
      "Evaluating  29  /  50\n",
      "Evaluating  30  /  50\n",
      "Evaluating  31  /  50\n",
      "Evaluating  32  /  50\n",
      "Evaluating  33  /  50\n",
      "Evaluating  34  /  50\n",
      "Evaluating  35  /  50\n",
      "Evaluating  36  /  50\n",
      "Evaluating  37  /  50\n",
      "Evaluating  38  /  50\n",
      "Evaluating  39  /  50\n",
      "Evaluating  40  /  50\n",
      "Evaluating  41  /  50\n",
      "Evaluating  42  /  50\n",
      "Evaluating  43  /  50\n",
      "Evaluating  44  /  50\n",
      "Evaluating  45  /  50\n",
      "Evaluating  46  /  50\n",
      "Evaluating  47  /  50\n",
      "Evaluating  48  /  50\n",
      "Evaluating  49  /  50\n",
      "Evaluating  50  /  50\n",
      "Evaluating  1  /  50\n",
      "Evaluating  2  /  50\n",
      "Evaluating  3  /  50\n",
      "Evaluating  4  /  50\n",
      "Evaluating  5  /  50\n",
      "Evaluating  6  /  50\n",
      "Evaluating  7  /  50\n",
      "Evaluating  8  /  50\n",
      "Evaluating  9  /  50\n",
      "Evaluating  10  /  50\n",
      "Evaluating  11  /  50\n",
      "Evaluating  12  /  50\n",
      "Evaluating  13  /  50\n",
      "Evaluating  14  /  50\n",
      "Evaluating  15  /  50\n",
      "Evaluating  16  /  50\n",
      "Evaluating  17  /  50\n",
      "Evaluating  18  /  50\n",
      "Evaluating  19  /  50\n",
      "Evaluating  20  /  50\n",
      "Evaluating  21  /  50\n",
      "Evaluating  22  /  50\n",
      "Evaluating  23  /  50\n",
      "Evaluating  24  /  50\n",
      "Evaluating  25  /  50\n",
      "Evaluating  26  /  50\n",
      "Evaluating  27  /  50\n",
      "Evaluating  28  /  50\n",
      "Evaluating  29  /  50\n",
      "Evaluating  30  /  50\n",
      "Evaluating  31  /  50\n",
      "Evaluating  32  /  50\n",
      "Evaluating  33  /  50\n",
      "Evaluating  34  /  50\n",
      "Evaluating  35  /  50\n",
      "Evaluating  36  /  50\n",
      "Evaluating  37  /  50\n",
      "Evaluating  38  /  50\n",
      "Evaluating  39  /  50\n",
      "Evaluating  40  /  50\n",
      "Evaluating  41  /  50\n",
      "Evaluating  42  /  50\n",
      "Evaluating  43  /  50\n",
      "Evaluating  44  /  50\n",
      "Evaluating  45  /  50\n",
      "Evaluating  46  /  50\n",
      "Evaluating  47  /  50\n",
      "Evaluating  48  /  50\n",
      "Evaluating  49  /  50\n",
      "Evaluating  50  /  50\n",
      "Evaluating  1  /  50\n",
      "Evaluating  2  /  50\n",
      "Evaluating  3  /  50\n",
      "Evaluating  4  /  50\n",
      "Evaluating  5  /  50\n",
      "Evaluating  6  /  50\n",
      "Evaluating  7  /  50\n",
      "Evaluating  8  /  50\n",
      "Evaluating  9  /  50\n",
      "Evaluating  10  /  50\n",
      "Evaluating  11  /  50\n",
      "Evaluating  12  /  50\n",
      "Evaluating  13  /  50\n",
      "Evaluating  14  /  50\n",
      "Evaluating  15  /  50\n",
      "Evaluating  16  /  50\n",
      "Evaluating  17  /  50\n",
      "Evaluating  18  /  50\n",
      "Evaluating  19  /  50\n",
      "Evaluating  20  /  50\n",
      "Evaluating  21  /  50\n",
      "Evaluating  22  /  50\n",
      "Evaluating  23  /  50\n",
      "Evaluating  24  /  50\n",
      "Evaluating  25  /  50\n",
      "Evaluating  26  /  50\n",
      "Evaluating  27  /  50\n",
      "Evaluating  28  /  50\n",
      "Evaluating  29  /  50\n",
      "Evaluating  30  /  50\n",
      "Evaluating  31  /  50\n",
      "Evaluating  32  /  50\n",
      "Evaluating  33  /  50\n",
      "Evaluating  34  /  50\n",
      "Evaluating  35  /  50\n",
      "Evaluating  36  /  50\n",
      "Evaluating  37  /  50\n",
      "Evaluating  38  /  50\n",
      "Evaluating  39  /  50\n",
      "Evaluating  40  /  50\n",
      "Evaluating  41  /  50\n",
      "Evaluating  42  /  50\n",
      "Evaluating  43  /  50\n",
      "Evaluating  44  /  50\n",
      "Evaluating  45  /  50\n",
      "Evaluating  46  /  50\n",
      "Evaluating  47  /  50\n",
      "Evaluating  48  /  50\n",
      "Evaluating  49  /  50\n",
      "Evaluating  50  /  50\n",
      "Evaluating  1  /  50\n",
      "Evaluating  2  /  50\n",
      "Evaluating  3  /  50\n",
      "Evaluating  4  /  50\n",
      "Evaluating  5  /  50\n",
      "Evaluating  6  /  50\n",
      "Evaluating  7  /  50\n",
      "Evaluating  8  /  50\n",
      "Evaluating  9  /  50\n",
      "Evaluating  10  /  50\n",
      "Evaluating  11  /  50\n",
      "Evaluating  12  /  50\n",
      "Evaluating  13  /  50\n",
      "Evaluating  14  /  50\n",
      "Evaluating  15  /  50\n",
      "Evaluating  16  /  50\n",
      "Evaluating  17  /  50\n",
      "Evaluating  18  /  50\n",
      "Evaluating  19  /  50\n",
      "Evaluating  20  /  50\n",
      "Evaluating  21  /  50\n",
      "Evaluating  22  /  50\n",
      "Evaluating  23  /  50\n",
      "Evaluating  24  /  50\n",
      "Evaluating  25  /  50\n",
      "Evaluating  26  /  50\n",
      "Evaluating  27  /  50\n",
      "Evaluating  28  /  50\n",
      "Evaluating  29  /  50\n",
      "Evaluating  30  /  50\n",
      "Evaluating  31  /  50\n",
      "Evaluating  32  /  50\n",
      "Evaluating  33  /  50\n",
      "Evaluating  34  /  50\n",
      "Evaluating  35  /  50\n",
      "Evaluating  36  /  50\n",
      "Evaluating  37  /  50\n",
      "Evaluating  38  /  50\n",
      "Evaluating  39  /  50\n",
      "Evaluating  40  /  50\n",
      "Evaluating  41  /  50\n",
      "Evaluating  42  /  50\n",
      "Evaluating  43  /  50\n",
      "Evaluating  44  /  50\n",
      "Evaluating  45  /  50\n",
      "Evaluating  46  /  50\n",
      "Evaluating  47  /  50\n",
      "Evaluating  48  /  50\n",
      "Evaluating  49  /  50\n",
      "Evaluating  50  /  50\n",
      "Evaluating  1  /  50\n",
      "Evaluating  2  /  50\n",
      "Evaluating  3  /  50\n",
      "Evaluating  4  /  50\n",
      "Evaluating  5  /  50\n",
      "Evaluating  6  /  50\n",
      "Evaluating  7  /  50\n",
      "Evaluating  8  /  50\n",
      "Evaluating  9  /  50\n",
      "Evaluating  10  /  50\n",
      "Evaluating  11  /  50\n",
      "Evaluating  12  /  50\n",
      "Evaluating  13  /  50\n",
      "Evaluating  14  /  50\n",
      "Evaluating  15  /  50\n",
      "Evaluating  16  /  50\n",
      "Evaluating  17  /  50\n",
      "Evaluating  18  /  50\n",
      "Evaluating  19  /  50\n",
      "Evaluating  20  /  50\n",
      "Evaluating  21  /  50\n",
      "Evaluating  22  /  50\n",
      "Evaluating  23  /  50\n",
      "Evaluating  24  /  50\n",
      "Evaluating  25  /  50\n",
      "Evaluating  26  /  50\n",
      "Evaluating  27  /  50\n",
      "Evaluating  28  /  50\n",
      "Evaluating  29  /  50\n",
      "Evaluating  30  /  50\n",
      "Evaluating  31  /  50\n",
      "Evaluating  32  /  50\n",
      "Evaluating  33  /  50\n",
      "Evaluating  34  /  50\n",
      "Evaluating  35  /  50\n",
      "Evaluating  36  /  50\n",
      "Evaluating  37  /  50\n",
      "Evaluating  38  /  50\n",
      "Evaluating  39  /  50\n",
      "Evaluating  40  /  50\n",
      "Evaluating  41  /  50\n",
      "Evaluating  42  /  50\n",
      "Evaluating  43  /  50\n",
      "Evaluating  44  /  50\n",
      "Evaluating  45  /  50\n",
      "Evaluating  46  /  50\n",
      "Evaluating  47  /  50\n",
      "Evaluating  48  /  50\n",
      "Evaluating  49  /  50\n",
      "Evaluating  50  /  50\n",
      "Evaluating  1  /  100\n",
      "Evaluating  2  /  100\n",
      "Evaluating  3  /  100\n",
      "Evaluating  4  /  100\n",
      "Evaluating  5  /  100\n",
      "Evaluating  6  /  100\n",
      "Evaluating  7  /  100\n",
      "Evaluating  8  /  100\n",
      "Evaluating  9  /  100\n",
      "Evaluating  10  /  100\n",
      "Evaluating  11  /  100\n",
      "Evaluating  12  /  100\n",
      "Evaluating  13  /  100\n",
      "Evaluating  14  /  100\n",
      "Evaluating  15  /  100\n",
      "Evaluating  16  /  100\n",
      "Evaluating  17  /  100\n",
      "Evaluating  18  /  100\n",
      "Evaluating  19  /  100\n",
      "Evaluating  20  /  100\n",
      "Evaluating  21  /  100\n",
      "Evaluating  22  /  100\n",
      "Evaluating  23  /  100\n",
      "Evaluating  24  /  100\n",
      "Evaluating  25  /  100\n",
      "Evaluating  26  /  100\n",
      "Evaluating  27  /  100\n",
      "Evaluating  28  /  100\n",
      "Evaluating  29  /  100\n",
      "Evaluating  30  /  100\n",
      "Evaluating  31  /  100\n",
      "Evaluating  32  /  100\n",
      "Evaluating  33  /  100\n",
      "Evaluating  34  /  100\n",
      "Evaluating  35  /  100\n",
      "Evaluating  36  /  100\n",
      "Evaluating  37  /  100\n",
      "Evaluating  38  /  100\n",
      "Evaluating  39  /  100\n",
      "Evaluating  40  /  100\n",
      "Evaluating  41  /  100\n",
      "Evaluating  42  /  100\n",
      "Evaluating  43  /  100\n",
      "Evaluating  44  /  100\n",
      "Evaluating  45  /  100\n",
      "Evaluating  46  /  100\n",
      "Evaluating  47  /  100\n",
      "Evaluating  48  /  100\n",
      "Evaluating  49  /  100\n",
      "Evaluating  50  /  100\n",
      "Evaluating  51  /  100\n",
      "Evaluating  52  /  100\n",
      "Evaluating  53  /  100\n",
      "Evaluating  54  /  100\n",
      "Evaluating  55  /  100\n",
      "Evaluating  56  /  100\n",
      "Evaluating  57  /  100\n",
      "Evaluating  58  /  100\n",
      "Evaluating  59  /  100\n",
      "Evaluating  60  /  100\n",
      "Evaluating  61  /  100\n",
      "Evaluating  62  /  100\n",
      "Evaluating  63  /  100\n",
      "Evaluating  64  /  100\n",
      "Evaluating  65  /  100\n",
      "Evaluating  66  /  100\n",
      "Evaluating  67  /  100\n",
      "Evaluating  68  /  100\n",
      "Evaluating  69  /  100\n",
      "Evaluating  70  /  100\n",
      "Evaluating  71  /  100\n",
      "Evaluating  72  /  100\n",
      "Evaluating  73  /  100\n",
      "Evaluating  74  /  100\n",
      "Evaluating  75  /  100\n",
      "Evaluating  76  /  100\n",
      "Evaluating  77  /  100\n",
      "Evaluating  78  /  100\n",
      "Evaluating  79  /  100\n",
      "Evaluating  80  /  100\n",
      "Evaluating  81  /  100\n",
      "Evaluating  82  /  100\n",
      "Evaluating  83  /  100\n",
      "Evaluating  84  /  100\n",
      "Evaluating  85  /  100\n",
      "Evaluating  86  /  100\n",
      "Evaluating  87  /  100\n",
      "Evaluating  88  /  100\n",
      "Evaluating  89  /  100\n",
      "Evaluating  90  /  100\n",
      "Evaluating  91  /  100\n",
      "Evaluating  92  /  100\n",
      "Evaluating  93  /  100\n",
      "Evaluating  94  /  100\n",
      "Evaluating  95  /  100\n",
      "Evaluating  96  /  100\n",
      "Evaluating  97  /  100\n",
      "Evaluating  98  /  100\n",
      "Evaluating  99  /  100\n",
      "Evaluating  100  /  100\n",
      "Evaluating  1  /  100\n",
      "Evaluating  2  /  100\n",
      "Evaluating  3  /  100\n",
      "Evaluating  4  /  100\n",
      "Evaluating  5  /  100\n",
      "Evaluating  6  /  100\n",
      "Evaluating  7  /  100\n",
      "Evaluating  8  /  100\n",
      "Evaluating  9  /  100\n",
      "Evaluating  10  /  100\n",
      "Evaluating  11  /  100\n",
      "Evaluating  12  /  100\n",
      "Evaluating  13  /  100\n",
      "Evaluating  14  /  100\n",
      "Evaluating  15  /  100\n",
      "Evaluating  16  /  100\n",
      "Evaluating  17  /  100\n",
      "Evaluating  18  /  100\n",
      "Evaluating  19  /  100\n",
      "Evaluating  20  /  100\n",
      "Evaluating  21  /  100\n",
      "Evaluating  22  /  100\n",
      "Evaluating  23  /  100\n",
      "Evaluating  24  /  100\n",
      "Evaluating  25  /  100\n",
      "Evaluating  26  /  100\n",
      "Evaluating  27  /  100\n",
      "Evaluating  28  /  100\n",
      "Evaluating  29  /  100\n",
      "Evaluating  30  /  100\n",
      "Evaluating  31  /  100\n",
      "Evaluating  32  /  100\n",
      "Evaluating  33  /  100\n",
      "Evaluating  34  /  100\n",
      "Evaluating  35  /  100\n",
      "Evaluating  36  /  100\n",
      "Evaluating  37  /  100\n",
      "Evaluating  38  /  100\n",
      "Evaluating  39  /  100\n",
      "Evaluating  40  /  100\n",
      "Evaluating  41  /  100\n",
      "Evaluating  42  /  100\n",
      "Evaluating  43  /  100\n",
      "Evaluating  44  /  100\n",
      "Evaluating  45  /  100\n",
      "Evaluating  46  /  100\n",
      "Evaluating  47  /  100\n",
      "Evaluating  48  /  100\n",
      "Evaluating  49  /  100\n",
      "Evaluating  50  /  100\n",
      "Evaluating  51  /  100\n",
      "Evaluating  52  /  100\n",
      "Evaluating  53  /  100\n",
      "Evaluating  54  /  100\n",
      "Evaluating  55  /  100\n",
      "Evaluating  56  /  100\n",
      "Evaluating  57  /  100\n",
      "Evaluating  58  /  100\n",
      "Evaluating  59  /  100\n",
      "Evaluating  60  /  100\n",
      "Evaluating  61  /  100\n",
      "Evaluating  62  /  100\n",
      "Evaluating  63  /  100\n",
      "Evaluating  64  /  100\n",
      "Evaluating  65  /  100\n",
      "Evaluating  66  /  100\n",
      "Evaluating  67  /  100\n",
      "Evaluating  68  /  100\n",
      "Evaluating  69  /  100\n",
      "Evaluating  70  /  100\n",
      "Evaluating  71  /  100\n",
      "Evaluating  72  /  100\n",
      "Evaluating  73  /  100\n",
      "Evaluating  74  /  100\n",
      "Evaluating  75  /  100\n",
      "Evaluating  76  /  100\n",
      "Evaluating  77  /  100\n",
      "Evaluating  78  /  100\n",
      "Evaluating  79  /  100\n",
      "Evaluating  80  /  100\n",
      "Evaluating  81  /  100\n",
      "Evaluating  82  /  100\n",
      "Evaluating  83  /  100\n",
      "Evaluating  84  /  100\n",
      "Evaluating  85  /  100\n",
      "Evaluating  86  /  100\n",
      "Evaluating  87  /  100\n",
      "Evaluating  88  /  100\n",
      "Evaluating  89  /  100\n",
      "Evaluating  90  /  100\n",
      "Evaluating  91  /  100\n",
      "Evaluating  92  /  100\n",
      "Evaluating  93  /  100\n",
      "Evaluating  94  /  100\n",
      "Evaluating  95  /  100\n",
      "Evaluating  96  /  100\n",
      "Evaluating  97  /  100\n",
      "Evaluating  98  /  100\n",
      "Evaluating  99  /  100\n",
      "Evaluating  100  /  100\n",
      "Evaluating  1  /  100\n",
      "Evaluating  2  /  100\n",
      "Evaluating  3  /  100\n",
      "Evaluating  4  /  100\n",
      "Evaluating  5  /  100\n",
      "Evaluating  6  /  100\n",
      "Evaluating  7  /  100\n",
      "Evaluating  8  /  100\n",
      "Evaluating  9  /  100\n",
      "Evaluating  10  /  100\n",
      "Evaluating  11  /  100\n",
      "Evaluating  12  /  100\n",
      "Evaluating  13  /  100\n",
      "Evaluating  14  /  100\n",
      "Evaluating  15  /  100\n",
      "Evaluating  16  /  100\n",
      "Evaluating  17  /  100\n",
      "Evaluating  18  /  100\n",
      "Evaluating  19  /  100\n",
      "Evaluating  20  /  100\n",
      "Evaluating  21  /  100\n",
      "Evaluating  22  /  100\n",
      "Evaluating  23  /  100\n",
      "Evaluating  24  /  100\n",
      "Evaluating  25  /  100\n",
      "Evaluating  26  /  100\n",
      "Evaluating  27  /  100\n",
      "Evaluating  28  /  100\n",
      "Evaluating  29  /  100\n",
      "Evaluating  30  /  100\n",
      "Evaluating  31  /  100\n",
      "Evaluating  32  /  100\n",
      "Evaluating  33  /  100\n",
      "Evaluating  34  /  100\n",
      "Evaluating  35  /  100\n",
      "Evaluating  36  /  100\n",
      "Evaluating  37  /  100\n",
      "Evaluating  38  /  100\n",
      "Evaluating  39  /  100\n",
      "Evaluating  40  /  100\n",
      "Evaluating  41  /  100\n",
      "Evaluating  42  /  100\n",
      "Evaluating  43  /  100\n",
      "Evaluating  44  /  100\n",
      "Evaluating  45  /  100\n",
      "Evaluating  46  /  100\n",
      "Evaluating  47  /  100\n",
      "Evaluating  48  /  100\n",
      "Evaluating  49  /  100\n",
      "Evaluating  50  /  100\n",
      "Evaluating  51  /  100\n",
      "Evaluating  52  /  100\n",
      "Evaluating  53  /  100\n",
      "Evaluating  54  /  100\n",
      "Evaluating  55  /  100\n",
      "Evaluating  56  /  100\n",
      "Evaluating  57  /  100\n",
      "Evaluating  58  /  100\n",
      "Evaluating  59  /  100\n",
      "Evaluating  60  /  100\n",
      "Evaluating  61  /  100\n",
      "Evaluating  62  /  100\n",
      "Evaluating  63  /  100\n",
      "Evaluating  64  /  100\n",
      "Evaluating  65  /  100\n",
      "Evaluating  66  /  100\n",
      "Evaluating  67  /  100\n",
      "Evaluating  68  /  100\n",
      "Evaluating  69  /  100\n",
      "Evaluating  70  /  100\n",
      "Evaluating  71  /  100\n",
      "Evaluating  72  /  100\n",
      "Evaluating  73  /  100\n",
      "Evaluating  74  /  100\n",
      "Evaluating  75  /  100\n",
      "Evaluating  76  /  100\n",
      "Evaluating  77  /  100\n",
      "Evaluating  78  /  100\n",
      "Evaluating  79  /  100\n",
      "Evaluating  80  /  100\n",
      "Evaluating  81  /  100\n",
      "Evaluating  82  /  100\n",
      "Evaluating  83  /  100\n",
      "Evaluating  84  /  100\n",
      "Evaluating  85  /  100\n",
      "Evaluating  86  /  100\n",
      "Evaluating  87  /  100\n",
      "Evaluating  88  /  100\n",
      "Evaluating  89  /  100\n",
      "Evaluating  90  /  100\n",
      "Evaluating  91  /  100\n",
      "Evaluating  92  /  100\n",
      "Evaluating  93  /  100\n",
      "Evaluating  94  /  100\n",
      "Evaluating  95  /  100\n",
      "Evaluating  96  /  100\n",
      "Evaluating  97  /  100\n",
      "Evaluating  98  /  100\n",
      "Evaluating  99  /  100\n",
      "Evaluating  100  /  100\n",
      "Evaluating  1  /  100\n",
      "Evaluating  2  /  100\n",
      "Evaluating  3  /  100\n",
      "Evaluating  4  /  100\n",
      "Evaluating  5  /  100\n",
      "Evaluating  6  /  100\n",
      "Evaluating  7  /  100\n",
      "Evaluating  8  /  100\n",
      "Evaluating  9  /  100\n",
      "Evaluating  10  /  100\n",
      "Evaluating  11  /  100\n",
      "Evaluating  12  /  100\n",
      "Evaluating  13  /  100\n",
      "Evaluating  14  /  100\n",
      "Evaluating  15  /  100\n",
      "Evaluating  16  /  100\n",
      "Evaluating  17  /  100\n",
      "Evaluating  18  /  100\n",
      "Evaluating  19  /  100\n",
      "Evaluating  20  /  100\n",
      "Evaluating  21  /  100\n",
      "Evaluating  22  /  100\n",
      "Evaluating  23  /  100\n",
      "Evaluating  24  /  100\n",
      "Evaluating  25  /  100\n",
      "Evaluating  26  /  100\n",
      "Evaluating  27  /  100\n",
      "Evaluating  28  /  100\n",
      "Evaluating  29  /  100\n",
      "Evaluating  30  /  100\n",
      "Evaluating  31  /  100\n",
      "Evaluating  32  /  100\n",
      "Evaluating  33  /  100\n",
      "Evaluating  34  /  100\n",
      "Evaluating  35  /  100\n",
      "Evaluating  36  /  100\n",
      "Evaluating  37  /  100\n",
      "Evaluating  38  /  100\n",
      "Evaluating  39  /  100\n",
      "Evaluating  40  /  100\n",
      "Evaluating  41  /  100\n",
      "Evaluating  42  /  100\n",
      "Evaluating  43  /  100\n",
      "Evaluating  44  /  100\n",
      "Evaluating  45  /  100\n",
      "Evaluating  46  /  100\n",
      "Evaluating  47  /  100\n",
      "Evaluating  48  /  100\n",
      "Evaluating  49  /  100\n",
      "Evaluating  50  /  100\n",
      "Evaluating  51  /  100\n",
      "Evaluating  52  /  100\n",
      "Evaluating  53  /  100\n",
      "Evaluating  54  /  100\n",
      "Evaluating  55  /  100\n",
      "Evaluating  56  /  100\n",
      "Evaluating  57  /  100\n",
      "Evaluating  58  /  100\n",
      "Evaluating  59  /  100\n",
      "Evaluating  60  /  100\n",
      "Evaluating  61  /  100\n",
      "Evaluating  62  /  100\n",
      "Evaluating  63  /  100\n",
      "Evaluating  64  /  100\n",
      "Evaluating  65  /  100\n",
      "Evaluating  66  /  100\n",
      "Evaluating  67  /  100\n",
      "Evaluating  68  /  100\n",
      "Evaluating  69  /  100\n",
      "Evaluating  70  /  100\n",
      "Evaluating  71  /  100\n",
      "Evaluating  72  /  100\n",
      "Evaluating  73  /  100\n",
      "Evaluating  74  /  100\n",
      "Evaluating  75  /  100\n",
      "Evaluating  76  /  100\n",
      "Evaluating  77  /  100\n",
      "Evaluating  78  /  100\n",
      "Evaluating  79  /  100\n",
      "Evaluating  80  /  100\n",
      "Evaluating  81  /  100\n",
      "Evaluating  82  /  100\n",
      "Evaluating  83  /  100\n",
      "Evaluating  84  /  100\n",
      "Evaluating  85  /  100\n",
      "Evaluating  86  /  100\n",
      "Evaluating  87  /  100\n",
      "Evaluating  88  /  100\n",
      "Evaluating  89  /  100\n",
      "Evaluating  90  /  100\n",
      "Evaluating  91  /  100\n",
      "Evaluating  92  /  100\n",
      "Evaluating  93  /  100\n",
      "Evaluating  94  /  100\n",
      "Evaluating  95  /  100\n",
      "Evaluating  96  /  100\n",
      "Evaluating  97  /  100\n",
      "Evaluating  98  /  100\n",
      "Evaluating  99  /  100\n",
      "Evaluating  100  /  100\n",
      "Evaluating  1  /  100\n",
      "Evaluating  2  /  100\n",
      "Evaluating  3  /  100\n",
      "Evaluating  4  /  100\n",
      "Evaluating  5  /  100\n",
      "Evaluating  6  /  100\n",
      "Evaluating  7  /  100\n",
      "Evaluating  8  /  100\n",
      "Evaluating  9  /  100\n",
      "Evaluating  10  /  100\n",
      "Evaluating  11  /  100\n",
      "Evaluating  12  /  100\n",
      "Evaluating  13  /  100\n",
      "Evaluating  14  /  100\n",
      "Evaluating  15  /  100\n",
      "Evaluating  16  /  100\n",
      "Evaluating  17  /  100\n",
      "Evaluating  18  /  100\n",
      "Evaluating  19  /  100\n",
      "Evaluating  20  /  100\n",
      "Evaluating  21  /  100\n",
      "Evaluating  22  /  100\n",
      "Evaluating  23  /  100\n",
      "Evaluating  24  /  100\n",
      "Evaluating  25  /  100\n",
      "Evaluating  26  /  100\n",
      "Evaluating  27  /  100\n",
      "Evaluating  28  /  100\n",
      "Evaluating  29  /  100\n",
      "Evaluating  30  /  100\n",
      "Evaluating  31  /  100\n",
      "Evaluating  32  /  100\n",
      "Evaluating  33  /  100\n",
      "Evaluating  34  /  100\n",
      "Evaluating  35  /  100\n",
      "Evaluating  36  /  100\n",
      "Evaluating  37  /  100\n",
      "Evaluating  38  /  100\n",
      "Evaluating  39  /  100\n",
      "Evaluating  40  /  100\n",
      "Evaluating  41  /  100\n",
      "Evaluating  42  /  100\n",
      "Evaluating  43  /  100\n",
      "Evaluating  44  /  100\n",
      "Evaluating  45  /  100\n",
      "Evaluating  46  /  100\n",
      "Evaluating  47  /  100\n",
      "Evaluating  48  /  100\n",
      "Evaluating  49  /  100\n",
      "Evaluating  50  /  100\n",
      "Evaluating  51  /  100\n",
      "Evaluating  52  /  100\n",
      "Evaluating  53  /  100\n",
      "Evaluating  54  /  100\n",
      "Evaluating  55  /  100\n",
      "Evaluating  56  /  100\n",
      "Evaluating  57  /  100\n",
      "Evaluating  58  /  100\n",
      "Evaluating  59  /  100\n",
      "Evaluating  60  /  100\n",
      "Evaluating  61  /  100\n",
      "Evaluating  62  /  100\n",
      "Evaluating  63  /  100\n",
      "Evaluating  64  /  100\n",
      "Evaluating  65  /  100\n",
      "Evaluating  66  /  100\n",
      "Evaluating  67  /  100\n",
      "Evaluating  68  /  100\n",
      "Evaluating  69  /  100\n",
      "Evaluating  70  /  100\n",
      "Evaluating  71  /  100\n",
      "Evaluating  72  /  100\n",
      "Evaluating  73  /  100\n",
      "Evaluating  74  /  100\n",
      "Evaluating  75  /  100\n",
      "Evaluating  76  /  100\n",
      "Evaluating  77  /  100\n",
      "Evaluating  78  /  100\n",
      "Evaluating  79  /  100\n",
      "Evaluating  80  /  100\n",
      "Evaluating  81  /  100\n",
      "Evaluating  82  /  100\n",
      "Evaluating  83  /  100\n",
      "Evaluating  84  /  100\n",
      "Evaluating  85  /  100\n",
      "Evaluating  86  /  100\n",
      "Evaluating  87  /  100\n",
      "Evaluating  88  /  100\n",
      "Evaluating  89  /  100\n",
      "Evaluating  90  /  100\n",
      "Evaluating  91  /  100\n",
      "Evaluating  92  /  100\n",
      "Evaluating  93  /  100\n",
      "Evaluating  94  /  100\n",
      "Evaluating  95  /  100\n",
      "Evaluating  96  /  100\n",
      "Evaluating  97  /  100\n",
      "Evaluating  98  /  100\n",
      "Evaluating  99  /  100\n",
      "Evaluating  100  /  100\n"
     ]
    }
   ],
   "source": [
    "#Run eval\n",
    "data[['score', 'data']] = data['tests'].apply(lambda x: llmEval(x, llmChain, 10, dataDesc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[list(data.columns).remove(['tests', 'data'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixEval(data):\n",
    "    newData = pd.DataFrame.from_dict(data, orient= 'index')[['top 5 sample', 'wordliststr', 'rawLLMOP', 'intruder', 'detectedIntruder', 'success']]\n",
    "    newData = newData.to_dict(orient='index')\n",
    "    tot = len(data)\n",
    "    i = 1\n",
    "    for k, v in newData.items():\n",
    "        i += 1\n",
    "        trueIntruder = newData[k]['intruder']\n",
    "        detectedIntruder = newData[k]['detectedIntruder']\n",
    "        llmOP = newData[k]['rawLLMOP']\n",
    "        # llmOP = 'is: int'\n",
    "        # trueIntruder = 'test'\n",
    "        # detectedIntruder = 'NOTDETECTED'\n",
    "#        print(trueIntruder)\n",
    "#        detectedIntruder = 'NOTDETECTED'\n",
    "        if detectedIntruder == 'NOTDETECTED':\n",
    "            print(\"fixing \",i,\" / \",tot)\n",
    "            regSearch = re.search(r\"is:\\W*([a-zA-Z]+)\\W*\", llmOP)\n",
    "            if regSearch:\n",
    "                detectedIntruder = regSearch.group(1)\n",
    "#        print(detectedIntruder)\n",
    "\n",
    "        newData[k]['detectedIntruder'] = detectedIntruder\n",
    "        if trueIntruder.lower() == detectedIntruder.lower():\n",
    "            newData[k]['success'] = 1\n",
    "        else:\n",
    "            newData[k]['success'] = 0\n",
    "\n",
    "    df = pd.DataFrame.from_dict(newData, orient= 'index')[['top 5 sample', 'wordliststr', 'rawLLMOP', 'intruder', 'detectedIntruder', 'success']]\n",
    "    rate = df['success'].sum() / len(df)\n",
    "    return pd.Series({'NewScore': rate, 'fixedData': newData})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fixing  9  /  20\n",
      "fixing  20  /  50\n",
      "fixing  24  /  50\n",
      "fixing  49  /  50\n",
      "fixing  6  /  50\n",
      "fixing  2  /  100\n",
      "fixing  33  /  100\n",
      "fixing  75  /  100\n",
      "fixing  83  /  100\n",
      "fixing  62  /  100\n",
      "fixing  17  /  100\n",
      "fixing  41  /  100\n",
      "fixing  60  /  100\n",
      "fixing  61  /  100\n"
     ]
    }
   ],
   "source": [
    "#Run repair\n",
    "data[['NewScore', 'fixedData']] = data['data'].apply(lambda x: fixEval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('evalOutputMallet2.csv')\n",
    "# pd.DataFrame.from_dict(dict(data.head(1)['data'])[0], orient= 'index')[['top 5 sample', 'wordliststr', 'rawLLMOP', 'intruder', 'detectedIntruder', 'success']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_excel('evalOutputMallet2.xlsx')\n",
    "# pd.DataFrame.from_dict(dict(data.head(1)['data'])[0], orient= 'index')[['top 5 sample', 'wordliststr', 'rawLLMOP', 'intruder', 'detectedIntruder', 'success']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_topics</th>\n",
       "      <th>seed</th>\n",
       "      <th>score</th>\n",
       "      <th>interval</th>\n",
       "      <th>burnin</th>\n",
       "      <th>tests</th>\n",
       "      <th>package</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>7097</td>\n",
       "      <td>0.20</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>{'0': {'top 5 sample': ['milk', 'feed', 'month...</td>\n",
       "      <td>mallet</td>\n",
       "      <td>{'0': {'top 5 sample': ['milk', 'feed', 'month...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>5301</td>\n",
       "      <td>0.70</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>{'0': {'top 5 sample': ['pregnant', 'day', 'we...</td>\n",
       "      <td>mallet</td>\n",
       "      <td>{'0': {'top 5 sample': ['pregnant', 'day', 'we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>8687</td>\n",
       "      <td>0.10</td>\n",
       "      <td>100</td>\n",
       "      <td>500</td>\n",
       "      <td>{'0': {'top 5 sample': ['breastfeed', 'milk', ...</td>\n",
       "      <td>mallet</td>\n",
       "      <td>{'0': {'top 5 sample': ['breastfeed', 'milk', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>3433</td>\n",
       "      <td>0.50</td>\n",
       "      <td>100</td>\n",
       "      <td>500</td>\n",
       "      <td>{'0': {'top 5 sample': ['time', 'make', 'orgas...</td>\n",
       "      <td>mallet</td>\n",
       "      <td>{'0': {'top 5 sample': ['time', 'make', 'orgas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>511</td>\n",
       "      <td>0.20</td>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "      <td>{'0': {'top 5 sample': ['abortion', 'mother', ...</td>\n",
       "      <td>mallet</td>\n",
       "      <td>{'0': {'top 5 sample': ['abortion', 'mother', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>7097</td>\n",
       "      <td>0.35</td>\n",
       "      <td>100</td>\n",
       "      <td>500</td>\n",
       "      <td>{'0': {'top 5 sample': ['day', 'make', 'help',...</td>\n",
       "      <td>mallet</td>\n",
       "      <td>{'0': {'top 5 sample': ['day', 'make', 'help',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20</td>\n",
       "      <td>5301</td>\n",
       "      <td>0.50</td>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "      <td>{'0': {'top 5 sample': ['time', 'start', 'feel...</td>\n",
       "      <td>mallet</td>\n",
       "      <td>{'0': {'top 5 sample': ['time', 'start', 'feel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20</td>\n",
       "      <td>8687</td>\n",
       "      <td>0.30</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>{'0': {'top 5 sample': ['baby', 'epidural', 'c...</td>\n",
       "      <td>mallet</td>\n",
       "      <td>{'0': {'top 5 sample': ['baby', 'epidural', 'c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20</td>\n",
       "      <td>3433</td>\n",
       "      <td>0.40</td>\n",
       "      <td>100</td>\n",
       "      <td>500</td>\n",
       "      <td>{'0': {'top 5 sample': ['feel', 'make', 'pregn...</td>\n",
       "      <td>mallet</td>\n",
       "      <td>{'0': {'top 5 sample': ['feel', 'make', 'pregn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20</td>\n",
       "      <td>511</td>\n",
       "      <td>0.40</td>\n",
       "      <td>100</td>\n",
       "      <td>500</td>\n",
       "      <td>{'0': {'top 5 sample': ['gain', 'pregnancy', '...</td>\n",
       "      <td>mallet</td>\n",
       "      <td>{'0': {'top 5 sample': ['gain', 'pregnancy', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>50</td>\n",
       "      <td>7097</td>\n",
       "      <td>0.22</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>{'0': {'top 5 sample': ['start', 'baby', 'day'...</td>\n",
       "      <td>mallet</td>\n",
       "      <td>{'0': {'top 5 sample': ['start', 'baby', 'day'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>50</td>\n",
       "      <td>5301</td>\n",
       "      <td>0.44</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>{'0': {'top 5 sample': ['test', 'problem', 'do...</td>\n",
       "      <td>mallet</td>\n",
       "      <td>{'0': {'top 5 sample': ['test', 'problem', 'do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>50</td>\n",
       "      <td>8687</td>\n",
       "      <td>0.34</td>\n",
       "      <td>100</td>\n",
       "      <td>500</td>\n",
       "      <td>{'0': {'top 5 sample': ['human', 'kill', 'pers...</td>\n",
       "      <td>mallet</td>\n",
       "      <td>{'0': {'top 5 sample': ['human', 'kill', 'pers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>50</td>\n",
       "      <td>3433</td>\n",
       "      <td>0.18</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>{'0': {'top 5 sample': ['antibiotic', 'yeast',...</td>\n",
       "      <td>mallet</td>\n",
       "      <td>{'0': {'top 5 sample': ['antibiotic', 'yeast',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>50</td>\n",
       "      <td>511</td>\n",
       "      <td>0.38</td>\n",
       "      <td>50</td>\n",
       "      <td>500</td>\n",
       "      <td>{'0': {'top 5 sample': ['pregnancy', 'stress',...</td>\n",
       "      <td>mallet</td>\n",
       "      <td>{'0': {'top 5 sample': ['pregnancy', 'stress',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>100</td>\n",
       "      <td>7097</td>\n",
       "      <td>0.29</td>\n",
       "      <td>100</td>\n",
       "      <td>500</td>\n",
       "      <td>{'0': {'top 5 sample': ['sexual_harassment', '...</td>\n",
       "      <td>mallet</td>\n",
       "      <td>{'0': {'top 5 sample': ['sexual_harassment', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>100</td>\n",
       "      <td>5301</td>\n",
       "      <td>0.20</td>\n",
       "      <td>50</td>\n",
       "      <td>200</td>\n",
       "      <td>{'0': {'top 5 sample': ['cervix', 'day', 'week...</td>\n",
       "      <td>mallet</td>\n",
       "      <td>{'0': {'top 5 sample': ['cervix', 'day', 'week...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>100</td>\n",
       "      <td>8687</td>\n",
       "      <td>0.22</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>{'0': {'top 5 sample': ['feel', 'lot', 'hurt',...</td>\n",
       "      <td>mallet</td>\n",
       "      <td>{'0': {'top 5 sample': ['feel', 'lot', 'hurt',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>100</td>\n",
       "      <td>3433</td>\n",
       "      <td>0.26</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>{'0': {'top 5 sample': ['pregnancy', 'baby', '...</td>\n",
       "      <td>mallet</td>\n",
       "      <td>{'0': {'top 5 sample': ['pregnancy', 'baby', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>100</td>\n",
       "      <td>511</td>\n",
       "      <td>0.30</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>{'0': {'top 5 sample': ['circumcision', 'cut',...</td>\n",
       "      <td>mallet</td>\n",
       "      <td>{'0': {'top 5 sample': ['circumcision', 'cut',...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    num_topics  seed  score  interval  burnin  \\\n",
       "0           10  7097   0.20         5     100   \n",
       "1           10  5301   0.70       100     100   \n",
       "2           10  8687   0.10       100     500   \n",
       "3           10  3433   0.50       100     500   \n",
       "4           10   511   0.20       100     200   \n",
       "5           20  7097   0.35       100     500   \n",
       "6           20  5301   0.50       100     200   \n",
       "7           20  8687   0.30        10     100   \n",
       "8           20  3433   0.40       100     500   \n",
       "9           20   511   0.40       100     500   \n",
       "10          50  7097   0.22       100     100   \n",
       "11          50  5301   0.44         5     100   \n",
       "12          50  8687   0.34       100     500   \n",
       "13          50  3433   0.18        50      50   \n",
       "14          50   511   0.38        50     500   \n",
       "15         100  7097   0.29       100     500   \n",
       "16         100  5301   0.20        50     200   \n",
       "17         100  8687   0.22        50     100   \n",
       "18         100  3433   0.26       100     100   \n",
       "19         100   511   0.30       100     100   \n",
       "\n",
       "                                                tests package  \\\n",
       "0   {'0': {'top 5 sample': ['milk', 'feed', 'month...  mallet   \n",
       "1   {'0': {'top 5 sample': ['pregnant', 'day', 'we...  mallet   \n",
       "2   {'0': {'top 5 sample': ['breastfeed', 'milk', ...  mallet   \n",
       "3   {'0': {'top 5 sample': ['time', 'make', 'orgas...  mallet   \n",
       "4   {'0': {'top 5 sample': ['abortion', 'mother', ...  mallet   \n",
       "5   {'0': {'top 5 sample': ['day', 'make', 'help',...  mallet   \n",
       "6   {'0': {'top 5 sample': ['time', 'start', 'feel...  mallet   \n",
       "7   {'0': {'top 5 sample': ['baby', 'epidural', 'c...  mallet   \n",
       "8   {'0': {'top 5 sample': ['feel', 'make', 'pregn...  mallet   \n",
       "9   {'0': {'top 5 sample': ['gain', 'pregnancy', '...  mallet   \n",
       "10  {'0': {'top 5 sample': ['start', 'baby', 'day'...  mallet   \n",
       "11  {'0': {'top 5 sample': ['test', 'problem', 'do...  mallet   \n",
       "12  {'0': {'top 5 sample': ['human', 'kill', 'pers...  mallet   \n",
       "13  {'0': {'top 5 sample': ['antibiotic', 'yeast',...  mallet   \n",
       "14  {'0': {'top 5 sample': ['pregnancy', 'stress',...  mallet   \n",
       "15  {'0': {'top 5 sample': ['sexual_harassment', '...  mallet   \n",
       "16  {'0': {'top 5 sample': ['cervix', 'day', 'week...  mallet   \n",
       "17  {'0': {'top 5 sample': ['feel', 'lot', 'hurt',...  mallet   \n",
       "18  {'0': {'top 5 sample': ['pregnancy', 'baby', '...  mallet   \n",
       "19  {'0': {'top 5 sample': ['circumcision', 'cut',...  mallet   \n",
       "\n",
       "                                                 data  \n",
       "0   {'0': {'top 5 sample': ['milk', 'feed', 'month...  \n",
       "1   {'0': {'top 5 sample': ['pregnant', 'day', 'we...  \n",
       "2   {'0': {'top 5 sample': ['breastfeed', 'milk', ...  \n",
       "3   {'0': {'top 5 sample': ['time', 'make', 'orgas...  \n",
       "4   {'0': {'top 5 sample': ['abortion', 'mother', ...  \n",
       "5   {'0': {'top 5 sample': ['day', 'make', 'help',...  \n",
       "6   {'0': {'top 5 sample': ['time', 'start', 'feel...  \n",
       "7   {'0': {'top 5 sample': ['baby', 'epidural', 'c...  \n",
       "8   {'0': {'top 5 sample': ['feel', 'make', 'pregn...  \n",
       "9   {'0': {'top 5 sample': ['gain', 'pregnancy', '...  \n",
       "10  {'0': {'top 5 sample': ['start', 'baby', 'day'...  \n",
       "11  {'0': {'top 5 sample': ['test', 'problem', 'do...  \n",
       "12  {'0': {'top 5 sample': ['human', 'kill', 'pers...  \n",
       "13  {'0': {'top 5 sample': ['antibiotic', 'yeast',...  \n",
       "14  {'0': {'top 5 sample': ['pregnancy', 'stress',...  \n",
       "15  {'0': {'top 5 sample': ['sexual_harassment', '...  \n",
       "16  {'0': {'top 5 sample': ['cervix', 'day', 'week...  \n",
       "17  {'0': {'top 5 sample': ['feel', 'lot', 'hurt',...  \n",
       "18  {'0': {'top 5 sample': ['pregnancy', 'baby', '...  \n",
       "19  {'0': {'top 5 sample': ['circumcision', 'cut',...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(50) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(dict(data.tail(1)['data'])[1], orient= 'index')[['top 5 sample', 'wordliststr', 'rawLLMOP', 'intruder', 'detectedIntruder', 'success']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Without dataset description\n",
    "# dataDesc = \"\"\"\"\"\"\n",
    "# dataDesc = \"\"\"\n",
    "# This is a dataset of words\n",
    "# \"\"\"\n",
    "# # dataDesc = \"\"\"\n",
    "# # The topic modelling is based on a corpus of reddit posts relating to maternal health. \n",
    "# # The data from which topics are extracted includes topics like abortion and sexuality.\n",
    "# # \"\"\"\n",
    "\n",
    "# for k, v in data.items():\n",
    "#     wordList = v['top 5 sample'] + [v['intruder']]\n",
    "#     random.shuffle(wordList)\n",
    "#     wordListStr = \", \".join(wordList)\n",
    "#     trueIntruder = v['intruder']\n",
    "#     detectedIntruder = 'NOTDETECTED'\n",
    "#     ans = llmChain({'datasetDescription': dataDesc, 'listOfWords': wordListStr})\n",
    "#     regSearch = re.search(r'000(.*?)000', ans['text'])\n",
    "#     data[k]['wordlist'] = wordList\n",
    "#     data[k]['wordliststr'] = wordListStr\n",
    "#     if regSearch:\n",
    "#         detectedIntruder = regSearch.group(1)\n",
    "#     data[k]['detectedIntruder'] = detectedIntruder\n",
    "#     data[k]['rawLLMOP'] = ans['text']\n",
    "#     if trueIntruder.lower() == detectedIntruder.lower():\n",
    "#         data[k]['success'] = 1\n",
    "#     else:\n",
    "#         data[k]['success'] = 0\n",
    "\n",
    "# df = pd.DataFrame.from_dict(data, orient='index')\n",
    "# rate = df['success'].sum() / len(df)\n",
    "# rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "##pd.DataFrame.from_dict(dict(data.head(1)['data'])[0], orient= 'index')\n",
    "#dict(data.head(1)[['data']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame.from_dict(data, orient= 'index')[['top 5 sample', 'wordliststr', 'rawLLMOP', 'intruder', 'detectedIntruder', 'success']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\n",
    "    'display.max_colwidth', 1000\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
